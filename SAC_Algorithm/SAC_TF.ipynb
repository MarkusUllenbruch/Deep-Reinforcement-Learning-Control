{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Soft Actor-Critic (SAC)\n",
    "### Balancing a Inverse Double Pendulum on Cart\n",
    "Implementation of newest style of SAC with two Q Networks and temperature parameter alpha optimization/tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp  # Distributions with reparam trick to gradients flow through\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers  # for layer weight initializing\n",
    "import tensorflow.keras.losses as losses\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, n_actions, input_shape, max_size=1000000):\n",
    "        self.mem_size = max_size\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "        self.n_count = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.n_count % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.n_count += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size=256):\n",
    "        max_mem = min(self.n_count, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, n_states, n_actions, fc1_dims, fc2_dims, network_name, chkpt_dir='tmp/SAC', init_w=3e-3):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.network_name = network_name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, network_name + '_SAC')\n",
    "\n",
    "        self.fc1 = Dense(units=fc1_dims, activation='relu', input_shape=(n_states + n_actions, ))\n",
    "        self.fc2 = Dense(units=fc2_dims, activation='relu')\n",
    "        self.q = Dense(units=1,\n",
    "                       kernel_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w), # Änderung\n",
    "                       bias_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w))\n",
    "\n",
    "    def call(self, state, action):\n",
    "        inputs = tf.concat([state, action], axis=1)\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        return self.q(x) \n",
    "\n",
    "\n",
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions,\n",
    "                 fc1_dims,\n",
    "                 fc2_dims,\n",
    "                 network_name,\n",
    "                 chkpt_dir='tmp/SAC',\n",
    "                 init_w=3e-3,\n",
    "                 log_std_min=-20,\n",
    "                 log_std_max=2):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.network_name = network_name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, network_name + '_SAC')\n",
    "        \n",
    "        self.fc1 = Dense(units=fc1_dims, activation='relu', input_shape=(n_states, ))\n",
    "        self.fc2 = Dense(units=fc2_dims, activation='relu')\n",
    "        \n",
    "        self.mu = Dense(units=n_actions,\n",
    "                      kernel_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                      bias_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w))\n",
    "        \n",
    "        self.log_std = Dense(units=n_actions,\n",
    "                      kernel_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                      bias_initializer=initializers.RandomUniform(minval=-init_w, maxval=init_w))\n",
    "        \n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        log_std = self.log_std(x)\n",
    "        log_std = tf.clip_by_value(log_std, clip_value_min=self.log_std_min, clip_value_max=self.log_std_max)\n",
    "        std = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mu, std)  # make Gaussian distribution of mu, and sigma for actions\n",
    "        \n",
    "        z = normal.sample()  # sample from distribution with reparam trick\n",
    "        action = tf.tanh(z)  # bound actions to [-1, +1]\n",
    "        \n",
    "        # correct log_probs because of bounding the actions\n",
    "        log_prob = normal.log_prob(z) - tf.math.log(1 - tf.math.square(action) + 1e-6)  # Ist dasselbe!\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=1, keepdims=True) # tf.reduce_sum() or tf.reduce_mean()\n",
    "\n",
    "        return action, log_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" 2019 State-of-the-Art Implementation of SAC with optimized temperature\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 lr_Q = 3e-4,\n",
    "                 lr_actor = 3e-4,\n",
    "                 lr_a = 3e-4,\n",
    "                 gamma=0.99,\n",
    "                 tau=0.005,\n",
    "                 layer1_size=256,\n",
    "                 layer2_size=256,\n",
    "                 batch_size=256,\n",
    "                 max_size=1000000,\n",
    "                 warmup=1000,\n",
    "                 policy_delay=1,\n",
    "                 minimum_entropy=None):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_range = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "        self.n_states = env.observation_space.shape[0]\n",
    "        self.n_actions = env.action_space.shape[0]\n",
    "\n",
    "        self.min_action = env.action_space.low\n",
    "        self.max_action = env.action_space.high\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup = warmup\n",
    "        self.time_step = 0\n",
    "        self.update_step = 0\n",
    "        self.policy_delay = policy_delay\n",
    "\n",
    "        self.policy_net = ActorNetwork(n_states=self.n_states, n_actions=self.n_actions,\n",
    "                                       fc1_dims=layer1_size, fc2_dims=layer2_size, network_name='Actor')\n",
    "\n",
    "        self.q_net1 = CriticNetwork(n_states=self.n_states, n_actions=self.n_actions,\n",
    "                                    fc1_dims=layer1_size, fc2_dims=layer2_size, network_name='Critic_1')\n",
    "\n",
    "        self.q_net2 = CriticNetwork(n_states=self.n_states, n_actions=self.n_actions,\n",
    "                                    fc1_dims=layer1_size, fc2_dims=layer2_size, network_name='Critic_2')\n",
    "\n",
    "        self.target_q_net1 = CriticNetwork(n_states=self.n_states, n_actions=self.n_actions,\n",
    "                                           fc1_dims=layer1_size, fc2_dims=layer2_size, network_name='Target_Critic_1')\n",
    "\n",
    "        self.target_q_net2 = CriticNetwork(n_states=self.n_states, n_actions=self.n_actions,\n",
    "                                           fc1_dims=layer1_size, fc2_dims=layer2_size, network_name='Target_Critic_2')\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(n_actions=self.n_actions,\n",
    "                                          input_shape=self.n_states,\n",
    "                                          max_size=max_size)\n",
    "        \n",
    "        self.policy_net.compile(optimizer=tf.keras.optimizers.Adam(lr=lr_actor))\n",
    "        self.q_net1.compile(optimizer=tf.keras.optimizers.Adam(lr=lr_Q))\n",
    "        self.q_net2.compile(optimizer=tf.keras.optimizers.Adam(lr=lr_Q))\n",
    "\n",
    "        self.update_target_network_parameters(tau=1)  # copy parameters to target networks\n",
    "\n",
    "        # entropy temperature parameter alpha\n",
    "        #self.log_alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        print(-tf.constant(env.action_space.shape[0], dtype=tf.float32))\n",
    "        \n",
    "        self.log_alpha = tf.Variable(tf.zeros(1), trainable=True)\n",
    "        self.minimum_entropy = -tf.reduce_prod(tf.convert_to_tensor(env.action_space.shape, dtype=tf.float32))\n",
    "        self.minimum_entropy = -tf.reduce_prod(tf.convert_to_tensor(env.action_space.shape, dtype=tf.float32)) if minimum_entropy is None else minimum_entropy\n",
    "        print('Minimum Entropy set to: ', self.minimum_entropy)\n",
    "        self.alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_a)\n",
    "        self.alpha = tf.exp(self.log_alpha).numpy()\n",
    "        print('alpha: ', self.alpha)\n",
    "        \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if self.time_step < self.warmup:\n",
    "            actions = np.random.uniform(low=-1.0, high=1.0, size=self.n_actions)  # \"random uniform distribution over all valid actions\"\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        else:\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            state = tf.expand_dims(state, axis=0)\n",
    "            actions, _ = self.policy_net(state)\n",
    "\n",
    "        self.time_step += 1\n",
    "        if self.time_step == self.warmup:\n",
    "            print('No warmup anymore!')\n",
    "        a = self.rescale_action(actions[0].numpy())\n",
    "        return a\n",
    "\n",
    "    def scale_action(self, action):\n",
    "        \"\"\" Scale all actions to [-1., +1.]\n",
    "\n",
    "        :param action: unscaled actions\n",
    "        :return: scaled actions all in range -1. .. +1.\n",
    "        \"\"\"\n",
    "        #old = 2 * (action - self.min_action) / (self.max_action - self.min_action) - 1.0\n",
    "        scale = (2 * action - (self.action_range[1] + self.action_range[0])) / (self.action_range[1] - self.action_range[0])\n",
    "        return scale\n",
    "\n",
    "    def rescale_action(self, action):\n",
    "        \"\"\" Rescale all scaled actions to environment actionspace values\n",
    "\n",
    "        :param action: scaled actions\n",
    "        :return: rescaled actions all in range min_action .. max_action\n",
    "        \"\"\"\n",
    "        #old = (action + 1.0) * (self.max_action - self.min_action) / 2.0 + self.min_action\n",
    "        rescale = action * (self.action_range[1] - self.action_range[0]) / 2.0 + \\\n",
    "               (self.action_range[1] + self.action_range[0]) / 2.0\n",
    "        return rescale\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        action = self.scale_action(action)  # ÄNDERUNG! Funktioniert das mit?\n",
    "        self.replay_buffer.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_target_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        weights = []\n",
    "        for theta_target, theta in zip(self.target_q_net1.get_weights(), \n",
    "                                       self.q_net1.get_weights()):\n",
    "            theta_target = tau*theta + (1-tau)*theta_target\n",
    "            weights.append(theta_target)\n",
    "        self.target_q_net1.set_weights(weights)\n",
    "        \n",
    "        weights = []\n",
    "        for theta_target, theta in zip(self.target_q_net2.get_weights(),\n",
    "                                       self.q_net2.get_weights()):\n",
    "            theta_target = tau*theta + (1-tau)*theta_target\n",
    "            weights.append(theta_target)\n",
    "        self.target_q_net2.set_weights(weights)\n",
    "        \n",
    "        #weights = []\n",
    "        #theta_target = self.target_q_net1.weights\n",
    "        #for i, theta in enumerate(self.q_net1.weights):\n",
    "        #    weights.append(tau*theta + (1-tau)*theta_target[i])\n",
    "        #self.target_q_net1.set_weights(weights)\n",
    "        # \n",
    "        #weights = []\n",
    "        #theta_target = self.target_q_net2.weights\n",
    "        #for i, theta in enumerate(self.q_net2.weights):\n",
    "        #    weights.append(tau*theta + (1-tau)*theta_target[i])\n",
    "        #self.target_q_net2.set_weights(weights)\n",
    "        \n",
    "\n",
    "    def save_models(self):\n",
    "        print('...save models...') # To Do!\n",
    "\n",
    "    def load_models(self):\n",
    "        print('...load models...') # To Do!\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_buffer.n_count < self.batch_size:\n",
    "            return\n",
    "        elif self.replay_buffer.n_count == self.batch_size:\n",
    "            print('Buffer Size equals batch Size! - Learning begins :)')\n",
    "            return\n",
    "        \n",
    "        # sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample_buffer(batch_size=self.batch_size)\n",
    "        \n",
    "        # convert batchs from 2D numpy arrays to tensorflow tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        \n",
    "        # expand rewards and dones from 1D numpy arrays to 2D tensors and reshape them\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        rewards = tf.expand_dims(rewards, axis=0)\n",
    "        rewards = tf.reshape(rewards, [self.batch_size, 1])\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        dones = tf.expand_dims(dones, axis=0)\n",
    "        dones = tf.reshape(dones, [self.batch_size, 1])\n",
    "        \n",
    "        \n",
    "        ## Update critic networks Q1 & Q2\n",
    "        with tf.GradientTape(persistent=True) as tape_Q:\n",
    "            next_actions, next_log_pi = self.policy_net(next_states)\n",
    "            Q1_next = self.target_q_net1(next_states, next_actions)\n",
    "            Q2_next = self.target_q_net2(next_states, next_actions)\n",
    "            next_q_target = tf.minimum(Q1_next, Q2_next) - self.alpha * next_log_pi\n",
    "            expected_q = tf.stop_gradient(rewards + (1 - dones) * self.gamma * next_q_target)\n",
    "            \n",
    "            curr_q1 = self.q_net1(states, actions)\n",
    "            curr_q2 = self.q_net2(states, actions)\n",
    "            \n",
    "            q1_loss = tf.reduce_mean((curr_q1 - expected_q)**2)\n",
    "            q2_loss = tf.reduce_mean((curr_q2 - expected_q)**2)  # tf.square()\n",
    "            q_loss = q1_loss + q2_loss\n",
    "        \n",
    "        grad_Q1 = tape_Q.gradient(q_loss, self.q_net1.trainable_variables)\n",
    "        grad_Q2 = tape_Q.gradient(q_loss, self.q_net2.trainable_variables)\n",
    "        \n",
    "        self.q_net1.optimizer.apply_gradients(zip(grad_Q1, self.q_net1.trainable_variables))\n",
    "        self.q_net2.optimizer.apply_gradients(zip(grad_Q2, self.q_net2.trainable_variables))\n",
    "        \n",
    "\n",
    "        ## Update policy network and polyak update target Q networks less frequently (like in TD3 --> Delayed SAC)\n",
    "        if self.update_step % self.policy_delay == 0:\n",
    "            with tf.GradientTape() as tape_policy:\n",
    "                new_actions, log_pi = self.policy_net(states)\n",
    "                Q1 = self.q_net1(states, new_actions)\n",
    "                Q2 = self.q_net2(states, new_actions)\n",
    "                Q_min = tf.minimum(Q1, Q2)\n",
    "                loss_policy = tf.reduce_mean(self.alpha * log_pi - Q_min)\n",
    "            \n",
    "            grad_policy = tape_policy.gradient(loss_policy, self.policy_net.trainable_variables)\n",
    "            self.policy_net.optimizer.apply_gradients(zip(grad_policy, self.policy_net.trainable_variables))\n",
    "\n",
    "            \n",
    "            self.update_target_network_parameters()  # update target networks\n",
    "\n",
    "        ## Update temperature\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, log_pi_a = self.policy_net(states)\n",
    "            alpha_loss = tf.reduce_mean(self.log_alpha*(-log_pi_a - self.minimum_entropy))\n",
    "            \n",
    "        grads = tape.gradient(alpha_loss, [self.log_alpha])\n",
    "        self.alpha_optimizer.apply_gradients(zip(grads, [self.log_alpha]))\n",
    "        self.alpha = tf.exp(self.log_alpha).numpy()\n",
    "\n",
    "        self.update_step += 1  # Keep track of the number of network updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, figure_file, Ylabel, color, avg_color=None , plot_folder='./plots', Xlabel='Episodes'):\n",
    "    if not os.path.exists(plot_folder):\n",
    "        os.makedirs(plot_folder)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i - 100):i + 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(scores, color=color)\n",
    "    if avg_color is not None:\n",
    "        plt.plot(running_avg, color=avg_color)\n",
    "    plt.xlabel(Xlabel)\n",
    "    plt.ylabel(Ylabel)\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-2.0, shape=(), dtype=float32)\n",
      "Minimum Entropy set to:  tf.Tensor(-2.0, shape=(), dtype=float32)\n",
      "alpha:  [1.]\n",
      "...save models...\n",
      "episode  0 score -67.3 avg score -67 steps  127 alpha  1.0\n",
      "Buffer Size equals batch Size! - Learning begins :)\n",
      "episode  1 score -97.8 avg score -83 steps  266 alpha  0.99988145\n",
      "episode  2 score -183.7 avg score -116 steps  379 alpha  0.9803062\n",
      "episode  3 score -249.5 avg score -150 steps  524 alpha  0.9456728\n",
      "episode  4 score -191.9 avg score -158 steps  655 alpha  0.9102256\n",
      "episode  5 score -97.9 avg score -148 steps  797 alpha  0.8766789\n",
      "episode  6 score -378.3 avg score -181 steps  899 alpha  0.8474386\n",
      "episode  7 score -455.6 avg score -215 steps  989 alpha  0.8251228\n",
      "No warmup anymore!\n",
      "episode  8 score -153.8 avg score -208 steps  1096 alpha  0.80275446\n",
      "episode  9 score -526.8 avg score -240 steps  1192 alpha  0.7806522\n",
      "episode  10 score -211.3 avg score -238 steps  1268 alpha  0.76147735\n",
      "episode  11 score -199.6 avg score -234 steps  1389 alpha  0.7401661\n",
      "episode  12 score -176.3 avg score -230 steps  1684 alpha  0.6985233\n",
      "episode  13 score -46.9 avg score -217 steps  1790 alpha  0.6612811\n",
      "episode  14 score -289.9 avg score -222 steps  2064 alpha  0.6279572\n",
      "episode  15 score -53.9 avg score -211 steps  2220 alpha  0.5917203\n",
      "episode  16 score -93.5 avg score -204 steps  2362 alpha  0.56808186\n",
      "episode  17 score -178.2 avg score -203 steps  2831 alpha  0.52298826\n",
      "episode  18 score -133.5 avg score -199 steps  3043 alpha  0.4769162\n",
      "episode  19 score -39.1 avg score -191 steps  3219 alpha  0.45294273\n",
      "episode  20 score -52.0 avg score -185 steps  4219 alpha  0.3904048\n",
      "episode  21 score -74.9 avg score -180 steps  4473 alpha  0.3339723\n",
      "episode  22 score -89.4 avg score -176 steps  4698 alpha  0.31546888\n",
      "episode  23 score -146.8 avg score -174 steps  4994 alpha  0.29611006\n",
      "episode  24 score -158.0 avg score -174 steps  5387 alpha  0.27274066\n",
      "episode  25 score -129.8 avg score -172 steps  5748 alpha  0.24890547\n",
      "episode  26 score -111.7 avg score -170 steps  6009 alpha  0.22997092\n",
      "episode  27 score -100.0 avg score -167 steps  6236 alpha  0.21592672\n",
      "episode  28 score -126.6 avg score -166 steps  6455 alpha  0.2043432\n",
      "episode  29 score -202.8 avg score -167 steps  7242 alpha  0.18151502\n",
      "episode  30 score -22.1 avg score -163 steps  7385 alpha  0.16180058\n",
      "episode  31 score -73.5 avg score -160 steps  8385 alpha  0.14056817\n",
      "episode  32 score 44.9 avg score -154 steps  9385 alpha  0.11239101\n",
      "episode  33 score -15.1 avg score -149 steps  10385 alpha  0.09735665\n",
      "episode  34 score -9.8 avg score -145 steps  11385 alpha  0.09549011\n",
      "episode  35 score -8.8 avg score -142 steps  12385 alpha  0.095616505\n",
      "episode  36 score -1.8 avg score -138 steps  13385 alpha  0.08918848\n",
      "episode  37 score -13.4 avg score -135 steps  14385 alpha  0.08922926\n",
      "episode  38 score -15.8 avg score -132 steps  15385 alpha  0.087884955\n",
      "episode  39 score -14.5 avg score -129 steps  16385 alpha  0.08302391\n",
      "episode  40 score -13.2 avg score -126 steps  17385 alpha  0.08059765\n",
      "episode  41 score -48.7 avg score -124 steps  18385 alpha  0.07471932\n",
      "episode  42 score -51.7 avg score -122 steps  19385 alpha  0.06536443\n",
      "episode  43 score -61.7 avg score -121 steps  20385 alpha  0.06979488\n",
      "episode  44 score -46.3 avg score -119 steps  21385 alpha  0.070014305\n",
      "episode  45 score -9.4 avg score -117 steps  22385 alpha  0.07294122\n",
      "episode  46 score -48.7 avg score -115 steps  23385 alpha  0.079352446\n",
      "episode  47 score -37.8 avg score -114 steps  24385 alpha  0.080332726\n",
      "episode  48 score 6.4 avg score -111 steps  25385 alpha  0.07883842\n",
      "episode  49 score -38.7 avg score -110 steps  26385 alpha  0.07668405\n",
      "episode  50 score 18.3 avg score -107 steps  26511 alpha  0.07758402\n",
      "episode  51 score -48.0 avg score -106 steps  27511 alpha  0.077646285\n",
      "episode  52 score -86.3 avg score -106 steps  28511 alpha  0.07533479\n",
      "episode  53 score -89.2 avg score -106 steps  29511 alpha  0.072598554\n",
      "episode  54 score -323.0 avg score -110 steps  29841 alpha  0.076079085\n",
      "episode  55 score -243.2 avg score -112 steps  30168 alpha  0.076153874\n",
      "episode  56 score -491.3 avg score -119 steps  30882 alpha  0.07212546\n",
      "episode  57 score -41.3 avg score -117 steps  31882 alpha  0.06893344\n",
      "episode  58 score -35.6 avg score -116 steps  32882 alpha  0.070726395\n",
      "episode  59 score -11.8 avg score -114 steps  33882 alpha  0.07087176\n",
      "episode  60 score -71.3 avg score -113 steps  34882 alpha  0.073698096\n",
      "episode  61 score -49.1 avg score -112 steps  35882 alpha  0.07827767\n",
      "episode  62 score -43.9 avg score -111 steps  36882 alpha  0.07419588\n",
      "episode  63 score -23.6 avg score -110 steps  37882 alpha  0.07389735\n",
      "episode  64 score -41.1 avg score -109 steps  38882 alpha  0.07778819\n",
      "episode  65 score -55.8 avg score -108 steps  39882 alpha  0.0783102\n",
      "episode  66 score -84.0 avg score -108 steps  40882 alpha  0.076456845\n",
      "episode  67 score -47.8 avg score -107 steps  41882 alpha  0.07062937\n",
      "episode  68 score -19.8 avg score -106 steps  42882 alpha  0.06416314\n",
      "episode  69 score -0.7 avg score -104 steps  43882 alpha  0.06242137\n",
      "episode  70 score -58.5 avg score -103 steps  44882 alpha  0.06837407\n",
      "episode  71 score -26.2 avg score -102 steps  45882 alpha  0.06766479\n",
      "episode  72 score -38.4 avg score -101 steps  46882 alpha  0.06330212\n",
      "episode  73 score 20.3 avg score -100 steps  47882 alpha  0.06285362\n",
      "episode  74 score -7.5 avg score -99 steps  48882 alpha  0.06242126\n",
      "episode  75 score 3.5 avg score -97 steps  49882 alpha  0.058904715\n",
      "episode  76 score 13.6 avg score -96 steps  50062 alpha  0.057336666\n",
      "episode  77 score -42.6 avg score -95 steps  51062 alpha  0.056532815\n",
      "episode  78 score 2.8 avg score -94 steps  52062 alpha  0.055613555\n",
      "episode  79 score 10.8 avg score -93 steps  52181 alpha  0.05637449\n",
      "episode  80 score -8.9 avg score -92 steps  52294 alpha  0.05655775\n",
      "episode  81 score -23.3 avg score -91 steps  53294 alpha  0.054396346\n",
      "episode  82 score -36.6 avg score -90 steps  54294 alpha  0.05278137\n",
      "episode  83 score -23.1 avg score -89 steps  55294 alpha  0.050809707\n",
      "episode  84 score -29.5 avg score -89 steps  56294 alpha  0.05017196\n",
      "episode  85 score -24.5 avg score -88 steps  57294 alpha  0.049526483\n",
      "episode  86 score -32.3 avg score -87 steps  58294 alpha  0.04823232\n",
      "episode  87 score -46.1 avg score -87 steps  59294 alpha  0.045961373\n",
      "episode  88 score 18.7 avg score -86 steps  60294 alpha  0.045082953\n",
      "episode  89 score -39.5 avg score -85 steps  61294 alpha  0.04387621\n",
      "episode  90 score -13.0 avg score -84 steps  62294 alpha  0.040771447\n",
      "episode  91 score -12.8 avg score -83 steps  63294 alpha  0.0399238\n",
      "episode  92 score -123.4 avg score -84 steps  64294 alpha  0.03856917\n",
      "episode  93 score -129.3 avg score -84 steps  65165 alpha  0.03873082\n",
      "episode  94 score -39.6 avg score -84 steps  66165 alpha  0.036412805\n",
      "episode  95 score -37.2 avg score -83 steps  67165 alpha  0.033756863\n",
      "episode  96 score -102.9 avg score -84 steps  67605 alpha  0.033773545\n",
      "episode  97 score -46.0 avg score -83 steps  68605 alpha  0.033695973\n",
      "episode  98 score -54.6 avg score -83 steps  69605 alpha  0.033237383\n",
      "episode  99 score -15.5 avg score -82 steps  70605 alpha  0.03199586\n",
      "episode  100 score -48.9 avg score -82 steps  71605 alpha  0.030069958\n",
      "episode  101 score -14.4 avg score -81 steps  72605 alpha  0.029915685\n",
      "episode  102 score -20.8 avg score -80 steps  73605 alpha  0.028641347\n",
      "episode  103 score -37.5 avg score -77 steps  74605 alpha  0.028597085\n",
      "episode  104 score -14.1 avg score -76 steps  75605 alpha  0.027755275\n",
      "episode  105 score 58.9 avg score -74 steps  76605 alpha  0.027842866\n",
      "episode  106 score 6.8 avg score -70 steps  76746 alpha  0.027689453\n",
      "...save models...\n",
      "episode  107 score -7.1 avg score -66 steps  77746 alpha  0.027885448\n",
      "...save models...\n",
      "episode  108 score 44.0 avg score -64 steps  78746 alpha  0.030468177\n",
      "...save models...\n",
      "episode  109 score 7.5 avg score -58 steps  79746 alpha  0.030318929\n",
      "...save models...\n",
      "episode  110 score 189.0 avg score -54 steps  80260 alpha  0.030364342\n",
      "...save models...\n",
      "episode  111 score 142.2 avg score -51 steps  81016 alpha  0.030713681\n",
      "...save models...\n",
      "episode  112 score 26.5 avg score -49 steps  82016 alpha  0.031326726\n",
      "...save models...\n",
      "episode  113 score -16.2 avg score -49 steps  83016 alpha  0.03040164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...save models...\n",
      "episode  114 score 131.6 avg score -45 steps  83916 alpha  0.030874776\n",
      "...save models...\n",
      "episode  115 score -26.1 avg score -44 steps  84916 alpha  0.02953326\n",
      "...save models...\n",
      "episode  116 score -47.0 avg score -44 steps  85916 alpha  0.029107803\n",
      "...save models...\n",
      "episode  117 score -12.2 avg score -42 steps  86916 alpha  0.02913047\n",
      "...save models...\n",
      "episode  118 score 40.1 avg score -40 steps  87916 alpha  0.029176759\n",
      "...save models...\n",
      "episode  119 score 16.6 avg score -40 steps  88916 alpha  0.031058097\n",
      "...save models...\n",
      "episode  120 score -43.4 avg score -40 steps  89916 alpha  0.031413935\n",
      "...save models...\n",
      "episode  121 score -41.7 avg score -39 steps  90916 alpha  0.032012265\n",
      "...save models...\n",
      "episode  122 score -3.5 avg score -39 steps  91916 alpha  0.032061722\n",
      "...save models...\n",
      "episode  123 score -3.6 avg score -37 steps  92916 alpha  0.030255893\n",
      "...save models...\n",
      "episode  124 score -4.2 avg score -36 steps  93916 alpha  0.029300047\n",
      "...save models...\n",
      "episode  125 score -12.3 avg score -34 steps  94916 alpha  0.029672546\n",
      "...save models...\n",
      "episode  126 score -32.7 avg score -34 steps  95916 alpha  0.030424537\n",
      "...save models...\n",
      "episode  127 score -3.0 avg score -33 steps  96916 alpha  0.030084755\n",
      "...save models...\n",
      "episode  128 score -72.7 avg score -32 steps  97916 alpha  0.0311432\n",
      "...save models...\n",
      "episode  129 score 7.3 avg score -30 steps  98916 alpha  0.030443693\n",
      "episode  130 score -38.8 avg score -30 steps  99916 alpha  0.030066771\n",
      "...save models...\n",
      "episode  131 score -17.1 avg score -30 steps  100916 alpha  0.029399522\n",
      "episode  132 score -27.0 avg score -30 steps  101916 alpha  0.029146692\n",
      "episode  133 score 12.3 avg score -30 steps  102916 alpha  0.028029278\n",
      "episode  134 score -4.2 avg score -30 steps  103916 alpha  0.028224988\n",
      "episode  135 score -141.9 avg score -31 steps  104904 alpha  0.028322244\n",
      "episode  136 score -38.3 avg score -32 steps  105151 alpha  0.027970202\n",
      "episode  137 score 27.2 avg score -31 steps  106151 alpha  0.027779207\n",
      "episode  138 score -66.4 avg score -32 steps  107151 alpha  0.02835521\n",
      "episode  139 score 45.0 avg score -31 steps  108151 alpha  0.027502362\n",
      "episode  140 score -18.6 avg score -31 steps  109151 alpha  0.027853468\n",
      "episode  141 score 20.4 avg score -31 steps  110151 alpha  0.028295387\n",
      "episode  142 score 14.9 avg score -30 steps  111151 alpha  0.028858509\n",
      "...save models...\n",
      "episode  143 score -7.0 avg score -29 steps  112151 alpha  0.029263511\n",
      "episode  144 score -91.6 avg score -30 steps  113151 alpha  0.029346932\n",
      "episode  145 score -13.4 avg score -30 steps  114151 alpha  0.028255165\n",
      "episode  146 score -26.5 avg score -30 steps  115151 alpha  0.02870948\n",
      "...save models...\n",
      "episode  147 score -6.8 avg score -29 steps  116151 alpha  0.028652603\n",
      "episode  148 score -46.5 avg score -30 steps  117151 alpha  0.027871545\n",
      "episode  149 score -14.8 avg score -30 steps  118151 alpha  0.02719025\n",
      "episode  150 score 2.3 avg score -30 steps  119151 alpha  0.02692571\n",
      "episode  151 score -41.4 avg score -30 steps  120151 alpha  0.027216109\n",
      "episode  152 score -51.9 avg score -29 steps  121151 alpha  0.02639119\n",
      "...save models...\n",
      "episode  153 score 2.2 avg score -28 steps  122151 alpha  0.025973327\n",
      "...save models...\n",
      "episode  154 score 22.6 avg score -25 steps  123151 alpha  0.025542788\n",
      "...save models...\n",
      "episode  155 score -20.5 avg score -23 steps  124151 alpha  0.026252035\n",
      "...save models...\n",
      "episode  156 score -10.6 avg score -18 steps  125151 alpha  0.026308786\n",
      "...save models...\n",
      "episode  157 score -35.2 avg score -18 steps  126151 alpha  0.025851537\n",
      "episode  158 score -43.3 avg score -18 steps  127151 alpha  0.025929578\n",
      "...save models...\n",
      "episode  159 score 12.5 avg score -18 steps  128151 alpha  0.025634903\n",
      "...save models...\n",
      "episode  160 score -16.4 avg score -17 steps  129151 alpha  0.025183331\n",
      "...save models...\n",
      "episode  161 score 43.8 avg score -16 steps  130151 alpha  0.02363852\n",
      "...save models...\n",
      "episode  162 score -18.4 avg score -16 steps  131151 alpha  0.024060424\n",
      "...save models...\n",
      "episode  163 score -22.3 avg score -16 steps  132151 alpha  0.02428656\n",
      "...save models...\n",
      "episode  164 score 2.7 avg score -16 steps  133151 alpha  0.024513932\n",
      "...save models...\n",
      "episode  165 score -39.4 avg score -15 steps  134151 alpha  0.023709182\n",
      "...save models...\n",
      "episode  166 score 26.5 avg score -14 steps  135151 alpha  0.02317629\n",
      "...save models...\n",
      "episode  167 score 5.7 avg score -14 steps  136151 alpha  0.023086427\n",
      "...save models...\n",
      "episode  168 score -11.5 avg score -14 steps  137151 alpha  0.02355704\n",
      "...save models...\n",
      "episode  169 score 27.3 avg score -13 steps  138151 alpha  0.024693366\n",
      "...save models...\n",
      "episode  170 score -6.3 avg score -13 steps  139151 alpha  0.025462493\n",
      "...save models...\n",
      "episode  171 score -0.3 avg score -13 steps  140151 alpha  0.024657171\n",
      "...save models...\n",
      "episode  172 score 3.9 avg score -12 steps  141151 alpha  0.025288705\n",
      "episode  173 score -7.5 avg score -12 steps  142151 alpha  0.025121182\n",
      "episode  174 score -27.5 avg score -13 steps  143151 alpha  0.023575656\n",
      "episode  175 score 9.8 avg score -13 steps  144151 alpha  0.022948407\n",
      "episode  176 score -11.8 avg score -13 steps  145151 alpha  0.023226863\n",
      "episode  177 score -30.9 avg score -13 steps  146151 alpha  0.02381721\n",
      "episode  178 score -35.8 avg score -13 steps  147151 alpha  0.023270972\n",
      "episode  179 score 12.2 avg score -13 steps  148151 alpha  0.022557296\n",
      "episode  180 score -18.4 avg score -13 steps  149151 alpha  0.023311935\n",
      "episode  181 score 13.1 avg score -13 steps  150151 alpha  0.022800067\n",
      "episode  182 score 8.9 avg score -12 steps  151151 alpha  0.02283046\n",
      "...save models...\n",
      "episode  183 score 2.0 avg score -12 steps  152151 alpha  0.022453735\n",
      "...save models...\n",
      "episode  184 score 6.5 avg score -12 steps  153151 alpha  0.022536049\n",
      "...save models...\n",
      "episode  185 score -19.3 avg score -12 steps  154151 alpha  0.022518234\n",
      "...save models...\n",
      "episode  186 score -7.0 avg score -11 steps  155151 alpha  0.022294102\n",
      "...save models...\n",
      "episode  187 score -23.7 avg score -11 steps  156151 alpha  0.021474529\n",
      "episode  188 score 10.9 avg score -11 steps  157151 alpha  0.022351766\n",
      "...save models...\n",
      "episode  189 score 2.1 avg score -11 steps  158151 alpha  0.021912368\n",
      "...save models...\n",
      "episode  190 score -2.5 avg score -11 steps  159151 alpha  0.02088974\n",
      "...save models...\n",
      "episode  191 score 37.5 avg score -10 steps  160151 alpha  0.020554084\n",
      "...save models...\n",
      "episode  192 score -7.9 avg score -9 steps  161151 alpha  0.020200161\n",
      "...save models...\n",
      "episode  193 score -4.3 avg score -8 steps  162151 alpha  0.02094104\n",
      "...save models...\n",
      "episode  194 score 0.6 avg score -7 steps  163151 alpha  0.02097334\n",
      "...save models...\n",
      "episode  195 score 22.7 avg score -7 steps  164151 alpha  0.02082641\n",
      "...save models...\n",
      "episode  196 score -1.1 avg score -6 steps  165151 alpha  0.021169005\n",
      "...save models...\n",
      "episode  197 score -28.1 avg score -6 steps  166151 alpha  0.020601703\n",
      "...save models...\n",
      "episode  198 score -21.2 avg score -5 steps  167151 alpha  0.021425385\n",
      "...save models...\n",
      "episode  199 score 0.0 avg score -5 steps  168151 alpha  0.020678086\n",
      "...save models...\n",
      "episode  200 score -13.7 avg score -5 steps  169151 alpha  0.021017466\n",
      "episode  201 score -15.5 avg score -5 steps  170151 alpha  0.020219743\n",
      "episode  202 score -31.8 avg score -5 steps  171151 alpha  0.020817097\n",
      "...save models...\n",
      "episode  203 score -7.1 avg score -5 steps  172151 alpha  0.022162871\n",
      "...save models...\n",
      "episode  204 score 52.8 avg score -4 steps  173151 alpha  0.020914173\n",
      "episode  205 score 8.9 avg score -4 steps  174151 alpha  0.020249577\n",
      "episode  206 score 3.7 avg score -5 steps  175151 alpha  0.019653903\n",
      "episode  207 score -10.7 avg score -5 steps  176151 alpha  0.020081546\n",
      "episode  208 score 23.1 avg score -5 steps  176267 alpha  0.019772403\n",
      "episode  209 score 15.1 avg score -5 steps  177267 alpha  0.019581255\n",
      "episode  210 score 5.3 avg score -7 steps  178267 alpha  0.019794941\n",
      "episode  211 score 127.7 avg score -7 steps  179156 alpha  0.0197439\n",
      "episode  212 score -2.3 avg score -7 steps  180156 alpha  0.020325536\n",
      "episode  213 score -62.6 avg score -7 steps  181156 alpha  0.019750541\n",
      "episode  214 score -25.5 avg score -9 steps  182156 alpha  0.019745635\n",
      "episode  215 score -51.2 avg score -9 steps  183156 alpha  0.020054478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  216 score 2.3 avg score -9 steps  184156 alpha  0.019265462\n",
      "episode  217 score -12.7 avg score -9 steps  185156 alpha  0.019992711\n",
      "episode  218 score -20.7 avg score -9 steps  186156 alpha  0.020088326\n",
      "episode  219 score 3.0 avg score -9 steps  187156 alpha  0.019450013\n",
      "episode  220 score -21.5 avg score -9 steps  188156 alpha  0.019926125\n",
      "episode  221 score -21.0 avg score -9 steps  189156 alpha  0.01987983\n",
      "episode  222 score -35.7 avg score -9 steps  190156 alpha  0.019561473\n",
      "episode  223 score -35.0 avg score -10 steps  191156 alpha  0.019610684\n",
      "episode  224 score -4.3 avg score -10 steps  192156 alpha  0.020049548\n",
      "episode  225 score -19.0 avg score -10 steps  193156 alpha  0.01947803\n",
      "episode  226 score 3.6 avg score -9 steps  194156 alpha  0.018820459\n",
      "episode  227 score -7.4 avg score -9 steps  195156 alpha  0.018066177\n",
      "episode  228 score 8.7 avg score -9 steps  196156 alpha  0.018324722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-49e1d0015a49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-10751739a673>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marku\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;31m# explicitly take priority.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[0mmask_arg_passed_by_framework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[0minput_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_input_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m     if (self._expects_mask_arg and input_masks is not None and\n\u001b[0;32m    680\u001b[0m         not self._call_arg_was_passed('mask', args, kwargs)):\n",
      "\u001b[1;32mC:\\Users\\marku\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_collect_input_masks\u001b[1;34m(self, inputs, args, kwargs)\u001b[0m\n\u001b[0;32m   1958\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_collect_input_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m     \u001b[1;34m\"\"\"Checks if `mask` argument was passed, else gathers mask from inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_arg_was_passed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1961\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_call_arg_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marku\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_call_arg_was_passed\u001b[1;34m(self, arg_name, args, kwargs, inputs_in_args)\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1975\u001b[1;33m     \u001b[0mcall_fn_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1976\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minputs_in_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m       \u001b[1;31m# Ignore `inputs` arg.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marku\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\tracking.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    417\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marku\\anaconda3\\envs\\master\\lib\\weakref.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learnID = '1'\n",
    "n_games = 300 #1500\n",
    "\n",
    "#RANDOM_SEED = 90\n",
    "\n",
    "#env_id = 'InvertedPendulumBulletEnv-v0'\n",
    "#env_id = 'Pendulum-v0'\n",
    "env_id = 'LunarLanderContinuous-v2'\n",
    "#env_id = 'Pendulum-v0'\n",
    "#env_id = 'BipedalWalker-v3'\n",
    "\n",
    "env = gym.make(env_id)\n",
    "\n",
    "#tf.random.set_seed(RANDOM_SEED)\n",
    "#env.seed(RANDOM_SEED)\n",
    "#env.action_space.seed(RANDOM_SEED)\n",
    "#np.random.seed(RANDOM_SEED)\n",
    "\n",
    "agent = Agent(env=env)\n",
    "\n",
    "filename_return = env_id + '_SAC_' + 'return_' + learnID\n",
    "filename_alpha = env_id + '_SAC_' + 'alpha_' + learnID\n",
    "figure_file_return = 'plots/' + filename_return\n",
    "figure_file_alpha = 'plots/' + filename_alpha\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "alpha_history = []\n",
    "\n",
    "steps = 0\n",
    "for i in range(n_games):\n",
    "    score = 0.0\n",
    "    alpha = []\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.remember(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        alpha.append(agent.alpha)\n",
    "\n",
    "    score_history.append(score)\n",
    "    alpha_history.append(np.mean(alpha))\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg score %1.f' % avg_score, 'steps ', steps, 'alpha ', np.mean(alpha))\n",
    "    \n",
    "plot_learning_curve(score_history, figure_file_return+'.png', color='lightgreen', avg_color='green', Ylabel='Return')\n",
    "plot_learning_curve(alpha_history, figure_file_alpha+'.png', color='blue', Ylabel='Temperature alpha')\n",
    "\n",
    "np.save(figure_file_return, score_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
